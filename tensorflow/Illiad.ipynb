{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jdaaa\\\\.keras\\\\datasets'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "  \n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)  \n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "  \n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=74, shape=(), dtype=string, numpy=b\"The sage Ulysses promis'd in thy tent:\">, <tf.Tensor: id=75, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=78, shape=(), dtype=string, numpy=b\"Jove to Olympus, to th' abode of Gods,\">, <tf.Tensor: id=79, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=82, shape=(), dtype=string, numpy=b'But what my single arm, and feet, and strength'>, <tf.Tensor: id=83, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=86, shape=(), dtype=string, numpy=b'\"Whither away? what madness fills your breasts?'>, <tf.Tensor: id=87, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=90, shape=(), dtype=string, numpy=b'spears in front of them, for Ajax had given them strict orders that no'>, <tf.Tensor: id=91, shape=(), dtype=int64, numpy=2>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17178"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"The sage Ulysses promis'd in thy tent:\"\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13150, 16448, 2534, 6586, 1103, 15989, 7088, 1091]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0629 19:40:07.549597  7296 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0629 19:40:07.565775  7296 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0629 19:40:07.570452 13748 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0629 19:40:07.579900 13748 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0629 19:40:07.587361  7296 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=149240, shape=(16,), dtype=int64, numpy=\n",
       " array([13150, 16448,  2534,  6586,  1103, 15989,  7088,  1091,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0], dtype=int64)>,\n",
       " <tf.Tensor: id=149244, shape=(), dtype=int64, numpy=1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One or more dense layers.\n",
    "# Edit the list in the `for` line to experiment with layer sizes.\n",
    "for units in [64, 64]:\n",
    "    model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "\n",
    "# Output layer. The first argument is the number of labels.\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0629 19:40:39.080477  5856 deprecation.py:323] From c:\\users\\jdaaa\\pycharmprojects\\textminingproject\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    169/Unknown - 32s 32s/step - loss: 1.1026 - accuracy: 0.20 - 32s 16s/step - loss: 1.1001 - accuracy: 0.29 - 32s 11s/step - loss: 1.0984 - accuracy: 0.31 - 33s 8s/step - loss: 1.0961 - accuracy: 0.3438 - 33s 7s/step - loss: 1.0936 - accuracy: 0.362 - 35s 6s/step - loss: 1.0914 - accuracy: 0.372 - 36s 5s/step - loss: 1.0887 - accuracy: 0.379 - 36s 5s/step - loss: 1.0871 - accuracy: 0.377 - 36s 4s/step - loss: 1.0861 - accuracy: 0.381 - 36s 4s/step - loss: 1.0823 - accuracy: 0.387 - 37s 3s/step - loss: 1.0786 - accuracy: 0.390 - 37s 3s/step - loss: 1.0758 - accuracy: 0.389 - 37s 3s/step - loss: 1.0734 - accuracy: 0.387 - 37s 3s/step - loss: 1.0700 - accuracy: 0.387 - 37s 2s/step - loss: 1.0680 - accuracy: 0.386 - 37s 2s/step - loss: 1.0629 - accuracy: 0.386 - 37s 2s/step - loss: 1.0559 - accuracy: 0.392 - 37s 2s/step - loss: 1.0536 - accuracy: 0.388 - 37s 2s/step - loss: 1.0490 - accuracy: 0.391 - 38s 2s/step - loss: 1.0457 - accuracy: 0.395 - 39s 2s/step - loss: 1.0419 - accuracy: 0.398 - 39s 2s/step - loss: 1.0384 - accuracy: 0.407 - 39s 2s/step - loss: 1.0321 - accuracy: 0.415 - 39s 2s/step - loss: 1.0274 - accuracy: 0.418 - 39s 2s/step - loss: 1.0196 - accuracy: 0.421 - 39s 1s/step - loss: 1.0134 - accuracy: 0.427 - 39s 1s/step - loss: 1.0064 - accuracy: 0.432 - 39s 1s/step - loss: 1.0002 - accuracy: 0.437 - 39s 1s/step - loss: 0.9908 - accuracy: 0.441 - 39s 1s/step - loss: 0.9849 - accuracy: 0.445 - 39s 1s/step - loss: 0.9895 - accuracy: 0.451 - 39s 1s/step - loss: 0.9792 - accuracy: 0.459 - 39s 1s/step - loss: 0.9754 - accuracy: 0.465 - 39s 1s/step - loss: 0.9671 - accuracy: 0.471 - 39s 1s/step - loss: 0.9587 - accuracy: 0.475 - 40s 1s/step - loss: 0.9510 - accuracy: 0.479 - 40s 1s/step - loss: 0.9418 - accuracy: 0.483 - 40s 1s/step - loss: 0.9359 - accuracy: 0.486 - 40s 1s/step - loss: 0.9298 - accuracy: 0.487 - 40s 995ms/step - loss: 0.9286 - accuracy: 0.48 - 40s 972ms/step - loss: 0.9211 - accuracy: 0.49 - 40s 950ms/step - loss: 0.9135 - accuracy: 0.49 - 40s 930ms/step - loss: 0.9106 - accuracy: 0.49 - 40s 910ms/step - loss: 0.9048 - accuracy: 0.49 - 40s 891ms/step - loss: 0.8980 - accuracy: 0.50 - 40s 874ms/step - loss: 0.8954 - accuracy: 0.50 - 40s 857ms/step - loss: 0.8921 - accuracy: 0.50 - 40s 840ms/step - loss: 0.8887 - accuracy: 0.50 - 40s 824ms/step - loss: 0.8830 - accuracy: 0.50 - 40s 809ms/step - loss: 0.8787 - accuracy: 0.51 - 41s 794ms/step - loss: 0.8756 - accuracy: 0.51 - 41s 780ms/step - loss: 0.8727 - accuracy: 0.51 - 41s 767ms/step - loss: 0.8711 - accuracy: 0.51 - 41s 754ms/step - loss: 0.8704 - accuracy: 0.51 - 41s 741ms/step - loss: 0.8661 - accuracy: 0.51 - 41s 729ms/step - loss: 0.8634 - accuracy: 0.52 - 41s 717ms/step - loss: 0.8593 - accuracy: 0.52 - 41s 706ms/step - loss: 0.8558 - accuracy: 0.52 - 41s 695ms/step - loss: 0.8522 - accuracy: 0.52 - 41s 685ms/step - loss: 0.8480 - accuracy: 0.52 - 41s 674ms/step - loss: 0.8446 - accuracy: 0.53 - 41s 664ms/step - loss: 0.8406 - accuracy: 0.53 - 41s 655ms/step - loss: 0.8371 - accuracy: 0.53 - 41s 646ms/step - loss: 0.8372 - accuracy: 0.53 - 41s 637ms/step - loss: 0.8344 - accuracy: 0.53 - 41s 628ms/step - loss: 0.8309 - accuracy: 0.53 - 42s 620ms/step - loss: 0.8293 - accuracy: 0.53 - 42s 612ms/step - loss: 0.8268 - accuracy: 0.54 - 42s 604ms/step - loss: 0.8252 - accuracy: 0.54 - 42s 597ms/step - loss: 0.8215 - accuracy: 0.54 - 42s 589ms/step - loss: 0.8186 - accuracy: 0.54 - 42s 582ms/step - loss: 0.8160 - accuracy: 0.54 - 42s 575ms/step - loss: 0.8144 - accuracy: 0.54 - 42s 568ms/step - loss: 0.8108 - accuracy: 0.54 - 42s 561ms/step - loss: 0.8096 - accuracy: 0.54 - 42s 555ms/step - loss: 0.8069 - accuracy: 0.55 - 42s 549ms/step - loss: 0.8030 - accuracy: 0.55 - 42s 543ms/step - loss: 0.8039 - accuracy: 0.55 - 42s 537ms/step - loss: 0.8011 - accuracy: 0.55 - 42s 531ms/step - loss: 0.8016 - accuracy: 0.55 - 43s 525ms/step - loss: 0.7986 - accuracy: 0.55 - 43s 519ms/step - loss: 0.7963 - accuracy: 0.55 - 43s 514ms/step - loss: 0.7948 - accuracy: 0.56 - 43s 508ms/step - loss: 0.7918 - accuracy: 0.56 - 43s 503ms/step - loss: 0.7900 - accuracy: 0.56 - 43s 498ms/step - loss: 0.7887 - accuracy: 0.56 - 43s 493ms/step - loss: 0.7870 - accuracy: 0.56 - 43s 488ms/step - loss: 0.7844 - accuracy: 0.56 - 43s 483ms/step - loss: 0.7820 - accuracy: 0.56 - 43s 479ms/step - loss: 0.7809 - accuracy: 0.56 - 43s 474ms/step - loss: 0.7782 - accuracy: 0.57 - 43s 470ms/step - loss: 0.7755 - accuracy: 0.57 - 43s 465ms/step - loss: 0.7730 - accuracy: 0.57 - 43s 461ms/step - loss: 0.7721 - accuracy: 0.57 - 43s 457ms/step - loss: 0.7701 - accuracy: 0.57 - 43s 453ms/step - loss: 0.7699 - accuracy: 0.57 - 44s 449ms/step - loss: 0.7698 - accuracy: 0.57 - 44s 445ms/step - loss: 0.7678 - accuracy: 0.57 - 44s 441ms/step - loss: 0.7658 - accuracy: 0.57 - 44s 437ms/step - loss: 0.7641 - accuracy: 0.57 - 44s 433ms/step - loss: 0.7631 - accuracy: 0.57 - 44s 430ms/step - loss: 0.7617 - accuracy: 0.57 - 44s 426ms/step - loss: 0.7614 - accuracy: 0.57 - 44s 423ms/step - loss: 0.7595 - accuracy: 0.58 - 44s 419ms/step - loss: 0.7574 - accuracy: 0.58 - 44s 416ms/step - loss: 0.7554 - accuracy: 0.58 - 44s 413ms/step - loss: 0.7538 - accuracy: 0.58 - 44s 409ms/step - loss: 0.7530 - accuracy: 0.58 - 44s 406ms/step - loss: 0.7517 - accuracy: 0.58 - 44s 403ms/step - loss: 0.7506 - accuracy: 0.58 - 44s 400ms/step - loss: 0.7491 - accuracy: 0.58 - 44s 397ms/step - loss: 0.7475 - accuracy: 0.58 - 45s 394ms/step - loss: 0.7464 - accuracy: 0.58 - 45s 391ms/step - loss: 0.7456 - accuracy: 0.58 - 45s 388ms/step - loss: 0.7446 - accuracy: 0.58 - 45s 385ms/step - loss: 0.7428 - accuracy: 0.59 - 45s 382ms/step - loss: 0.7412 - accuracy: 0.59 - 45s 379ms/step - loss: 0.7404 - accuracy: 0.59 - 45s 376ms/step - loss: 0.7394 - accuracy: 0.59 - 45s 374ms/step - loss: 0.7380 - accuracy: 0.59 - 45s 371ms/step - loss: 0.7377 - accuracy: 0.59 - 45s 369ms/step - loss: 0.7360 - accuracy: 0.59 - 45s 366ms/step - loss: 0.7363 - accuracy: 0.59 - 45s 364ms/step - loss: 0.7358 - accuracy: 0.59 - 45s 361ms/step - loss: 0.7339 - accuracy: 0.59 - 45s 359ms/step - loss: 0.7320 - accuracy: 0.59 - 45s 357ms/step - loss: 0.7304 - accuracy: 0.59 - 45s 354ms/step - loss: 0.7288 - accuracy: 0.60 - 45s 352ms/step - loss: 0.7277 - accuracy: 0.60 - 45s 350ms/step - loss: 0.7278 - accuracy: 0.60 - 46s 348ms/step - loss: 0.7271 - accuracy: 0.60 - 46s 346ms/step - loss: 0.7262 - accuracy: 0.60 - 46s 343ms/step - loss: 0.7250 - accuracy: 0.60 - 46s 341ms/step - loss: 0.7247 - accuracy: 0.60 - 46s 339ms/step - loss: 0.7246 - accuracy: 0.60 - 46s 337ms/step - loss: 0.7244 - accuracy: 0.60 - 46s 335ms/step - loss: 0.7233 - accuracy: 0.60 - 46s 333ms/step - loss: 0.7232 - accuracy: 0.60 - 46s 331ms/step - loss: 0.7223 - accuracy: 0.60 - 46s 329ms/step - loss: 0.7214 - accuracy: 0.60 - 46s 327ms/step - loss: 0.7207 - accuracy: 0.60 - 46s 325ms/step - loss: 0.7196 - accuracy: 0.60 - 46s 323ms/step - loss: 0.7195 - accuracy: 0.60 - 46s 322ms/step - loss: 0.7186 - accuracy: 0.60 - 46s 320ms/step - loss: 0.7173 - accuracy: 0.60 - 46s 318ms/step - loss: 0.7159 - accuracy: 0.60 - 46s 316ms/step - loss: 0.7145 - accuracy: 0.61 - 47s 314ms/step - loss: 0.7139 - accuracy: 0.61 - 47s 313ms/step - loss: 0.7136 - accuracy: 0.61 - 47s 311ms/step - loss: 0.7132 - accuracy: 0.61 - 47s 309ms/step - loss: 0.7121 - accuracy: 0.61 - 47s 307ms/step - loss: 0.7113 - accuracy: 0.61 - 47s 306ms/step - loss: 0.7106 - accuracy: 0.61 - 47s 304ms/step - loss: 0.7095 - accuracy: 0.61 - 47s 303ms/step - loss: 0.7085 - accuracy: 0.61 - 47s 301ms/step - loss: 0.7078 - accuracy: 0.61 - 47s 300ms/step - loss: 0.7076 - accuracy: 0.61 - 47s 298ms/step - loss: 0.7068 - accuracy: 0.61 - 47s 296ms/step - loss: 0.7060 - accuracy: 0.61 - 47s 295ms/step - loss: 0.7059 - accuracy: 0.61 - 47s 294ms/step - loss: 0.7063 - accuracy: 0.61 - 47s 292ms/step - loss: 0.7057 - accuracy: 0.61 - 47s 291ms/step - loss: 0.7048 - accuracy: 0.61 - 47s 289ms/step - loss: 0.7041 - accuracy: 0.61 - 48s 288ms/step - loss: 0.7037 - accuracy: 0.61 - 48s 287ms/step - loss: 0.7024 - accuracy: 0.61 - 48s 285ms/step - loss: 0.7016 - accuracy: 0.61 - 48s 284ms/step - loss: 0.7011 - accuracy: 0.61 - 48s 283ms/step - loss: 0.7005 - accuracy: 0.6199"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    336/Unknown - 48s 281ms/step - loss: 0.7000 - accuracy: 0.62 - 48s 280ms/step - loss: 0.7004 - accuracy: 0.62 - 48s 278ms/step - loss: 0.6993 - accuracy: 0.62 - 48s 277ms/step - loss: 0.6984 - accuracy: 0.62 - 48s 276ms/step - loss: 0.6975 - accuracy: 0.62 - 48s 275ms/step - loss: 0.6965 - accuracy: 0.62 - 48s 273ms/step - loss: 0.6961 - accuracy: 0.62 - 48s 272ms/step - loss: 0.6958 - accuracy: 0.62 - 48s 271ms/step - loss: 0.6953 - accuracy: 0.62 - 48s 270ms/step - loss: 0.6949 - accuracy: 0.62 - 48s 268ms/step - loss: 0.6944 - accuracy: 0.62 - 48s 267ms/step - loss: 0.6936 - accuracy: 0.62 - 48s 266ms/step - loss: 0.6927 - accuracy: 0.62 - 48s 265ms/step - loss: 0.6920 - accuracy: 0.62 - 49s 264ms/step - loss: 0.6908 - accuracy: 0.62 - 49s 263ms/step - loss: 0.6904 - accuracy: 0.62 - 49s 261ms/step - loss: 0.6898 - accuracy: 0.62 - 49s 260ms/step - loss: 0.6907 - accuracy: 0.62 - 49s 259ms/step - loss: 0.6906 - accuracy: 0.62 - 49s 258ms/step - loss: 0.6904 - accuracy: 0.62 - 49s 257ms/step - loss: 0.6899 - accuracy: 0.62 - 49s 256ms/step - loss: 0.6897 - accuracy: 0.62 - 49s 255ms/step - loss: 0.6886 - accuracy: 0.62 - 49s 254ms/step - loss: 0.6878 - accuracy: 0.62 - 49s 253ms/step - loss: 0.6870 - accuracy: 0.62 - 49s 252ms/step - loss: 0.6864 - accuracy: 0.62 - 49s 251ms/step - loss: 0.6865 - accuracy: 0.63 - 49s 250ms/step - loss: 0.6859 - accuracy: 0.63 - 49s 249ms/step - loss: 0.6853 - accuracy: 0.63 - 49s 248ms/step - loss: 0.6851 - accuracy: 0.63 - 49s 247ms/step - loss: 0.6843 - accuracy: 0.63 - 49s 246ms/step - loss: 0.6839 - accuracy: 0.63 - 50s 245ms/step - loss: 0.6833 - accuracy: 0.63 - 50s 244ms/step - loss: 0.6828 - accuracy: 0.63 - 50s 243ms/step - loss: 0.6819 - accuracy: 0.63 - 50s 243ms/step - loss: 0.6814 - accuracy: 0.63 - 50s 242ms/step - loss: 0.6816 - accuracy: 0.63 - 50s 241ms/step - loss: 0.6803 - accuracy: 0.63 - 50s 240ms/step - loss: 0.6789 - accuracy: 0.63 - 50s 239ms/step - loss: 0.6791 - accuracy: 0.63 - 50s 238ms/step - loss: 0.6783 - accuracy: 0.63 - 50s 237ms/step - loss: 0.6776 - accuracy: 0.63 - 50s 236ms/step - loss: 0.6774 - accuracy: 0.63 - 50s 236ms/step - loss: 0.6764 - accuracy: 0.63 - 50s 235ms/step - loss: 0.6754 - accuracy: 0.63 - 50s 234ms/step - loss: 0.6745 - accuracy: 0.63 - 50s 233ms/step - loss: 0.6747 - accuracy: 0.63 - 50s 232ms/step - loss: 0.6742 - accuracy: 0.63 - 50s 232ms/step - loss: 0.6735 - accuracy: 0.64 - 51s 231ms/step - loss: 0.6729 - accuracy: 0.64 - 51s 230ms/step - loss: 0.6722 - accuracy: 0.64 - 51s 229ms/step - loss: 0.6722 - accuracy: 0.64 - 51s 228ms/step - loss: 0.6725 - accuracy: 0.64 - 51s 228ms/step - loss: 0.6712 - accuracy: 0.64 - 51s 227ms/step - loss: 0.6705 - accuracy: 0.64 - 51s 226ms/step - loss: 0.6701 - accuracy: 0.64 - 51s 225ms/step - loss: 0.6689 - accuracy: 0.64 - 51s 225ms/step - loss: 0.6682 - accuracy: 0.64 - 51s 224ms/step - loss: 0.6674 - accuracy: 0.64 - 51s 223ms/step - loss: 0.6671 - accuracy: 0.64 - 51s 223ms/step - loss: 0.6662 - accuracy: 0.64 - 51s 222ms/step - loss: 0.6662 - accuracy: 0.64 - 51s 221ms/step - loss: 0.6652 - accuracy: 0.64 - 51s 220ms/step - loss: 0.6647 - accuracy: 0.64 - 51s 220ms/step - loss: 0.6638 - accuracy: 0.64 - 51s 219ms/step - loss: 0.6632 - accuracy: 0.64 - 52s 218ms/step - loss: 0.6625 - accuracy: 0.64 - 52s 218ms/step - loss: 0.6623 - accuracy: 0.64 - 52s 217ms/step - loss: 0.6613 - accuracy: 0.64 - 52s 216ms/step - loss: 0.6611 - accuracy: 0.64 - 52s 216ms/step - loss: 0.6605 - accuracy: 0.64 - 52s 215ms/step - loss: 0.6606 - accuracy: 0.64 - 52s 214ms/step - loss: 0.6597 - accuracy: 0.65 - 52s 214ms/step - loss: 0.6595 - accuracy: 0.65 - 52s 213ms/step - loss: 0.6596 - accuracy: 0.65 - 52s 212ms/step - loss: 0.6592 - accuracy: 0.65 - 52s 212ms/step - loss: 0.6589 - accuracy: 0.65 - 52s 211ms/step - loss: 0.6578 - accuracy: 0.65 - 52s 211ms/step - loss: 0.6573 - accuracy: 0.65 - 52s 210ms/step - loss: 0.6568 - accuracy: 0.65 - 52s 209ms/step - loss: 0.6565 - accuracy: 0.65 - 52s 209ms/step - loss: 0.6556 - accuracy: 0.65 - 52s 208ms/step - loss: 0.6549 - accuracy: 0.65 - 53s 208ms/step - loss: 0.6541 - accuracy: 0.65 - 53s 207ms/step - loss: 0.6538 - accuracy: 0.65 - 53s 206ms/step - loss: 0.6531 - accuracy: 0.65 - 53s 206ms/step - loss: 0.6528 - accuracy: 0.65 - 53s 205ms/step - loss: 0.6524 - accuracy: 0.65 - 53s 205ms/step - loss: 0.6515 - accuracy: 0.65 - 53s 204ms/step - loss: 0.6517 - accuracy: 0.65 - 53s 203ms/step - loss: 0.6512 - accuracy: 0.65 - 53s 203ms/step - loss: 0.6512 - accuracy: 0.65 - 53s 202ms/step - loss: 0.6508 - accuracy: 0.65 - 53s 202ms/step - loss: 0.6505 - accuracy: 0.65 - 53s 201ms/step - loss: 0.6500 - accuracy: 0.65 - 53s 200ms/step - loss: 0.6492 - accuracy: 0.65 - 53s 200ms/step - loss: 0.6490 - accuracy: 0.65 - 53s 199ms/step - loss: 0.6483 - accuracy: 0.65 - 53s 199ms/step - loss: 0.6482 - accuracy: 0.66 - 53s 198ms/step - loss: 0.6476 - accuracy: 0.66 - 53s 198ms/step - loss: 0.6471 - accuracy: 0.66 - 53s 197ms/step - loss: 0.6462 - accuracy: 0.66 - 54s 197ms/step - loss: 0.6458 - accuracy: 0.66 - 54s 196ms/step - loss: 0.6452 - accuracy: 0.66 - 54s 196ms/step - loss: 0.6449 - accuracy: 0.66 - 54s 195ms/step - loss: 0.6439 - accuracy: 0.66 - 54s 195ms/step - loss: 0.6439 - accuracy: 0.66 - 54s 194ms/step - loss: 0.6441 - accuracy: 0.66 - 54s 194ms/step - loss: 0.6432 - accuracy: 0.66 - 54s 193ms/step - loss: 0.6427 - accuracy: 0.66 - 54s 193ms/step - loss: 0.6419 - accuracy: 0.66 - 54s 192ms/step - loss: 0.6415 - accuracy: 0.66 - 54s 192ms/step - loss: 0.6412 - accuracy: 0.66 - 54s 191ms/step - loss: 0.6409 - accuracy: 0.66 - 54s 191ms/step - loss: 0.6403 - accuracy: 0.66 - 54s 190ms/step - loss: 0.6400 - accuracy: 0.66 - 54s 190ms/step - loss: 0.6396 - accuracy: 0.66 - 54s 189ms/step - loss: 0.6392 - accuracy: 0.66 - 54s 189ms/step - loss: 0.6384 - accuracy: 0.66 - 54s 188ms/step - loss: 0.6375 - accuracy: 0.66 - 54s 188ms/step - loss: 0.6370 - accuracy: 0.66 - 55s 187ms/step - loss: 0.6366 - accuracy: 0.66 - 55s 187ms/step - loss: 0.6363 - accuracy: 0.66 - 55s 186ms/step - loss: 0.6357 - accuracy: 0.66 - 55s 186ms/step - loss: 0.6356 - accuracy: 0.66 - 55s 186ms/step - loss: 0.6349 - accuracy: 0.67 - 55s 185ms/step - loss: 0.6345 - accuracy: 0.67 - 55s 185ms/step - loss: 0.6342 - accuracy: 0.67 - 55s 184ms/step - loss: 0.6340 - accuracy: 0.67 - 55s 184ms/step - loss: 0.6333 - accuracy: 0.67 - 55s 183ms/step - loss: 0.6329 - accuracy: 0.67 - 55s 183ms/step - loss: 0.6322 - accuracy: 0.67 - 55s 183ms/step - loss: 0.6317 - accuracy: 0.67 - 55s 182ms/step - loss: 0.6309 - accuracy: 0.67 - 55s 182ms/step - loss: 0.6301 - accuracy: 0.67 - 55s 181ms/step - loss: 0.6300 - accuracy: 0.67 - 55s 181ms/step - loss: 0.6297 - accuracy: 0.67 - 55s 181ms/step - loss: 0.6294 - accuracy: 0.67 - 56s 180ms/step - loss: 0.6287 - accuracy: 0.67 - 56s 180ms/step - loss: 0.6285 - accuracy: 0.67 - 56s 179ms/step - loss: 0.6278 - accuracy: 0.67 - 56s 179ms/step - loss: 0.6276 - accuracy: 0.67 - 56s 179ms/step - loss: 0.6271 - accuracy: 0.67 - 56s 178ms/step - loss: 0.6267 - accuracy: 0.67 - 56s 178ms/step - loss: 0.6261 - accuracy: 0.67 - 56s 177ms/step - loss: 0.6262 - accuracy: 0.67 - 56s 177ms/step - loss: 0.6262 - accuracy: 0.67 - 56s 177ms/step - loss: 0.6255 - accuracy: 0.67 - 56s 176ms/step - loss: 0.6247 - accuracy: 0.67 - 56s 176ms/step - loss: 0.6245 - accuracy: 0.67 - 56s 175ms/step - loss: 0.6239 - accuracy: 0.67 - 56s 175ms/step - loss: 0.6231 - accuracy: 0.67 - 56s 175ms/step - loss: 0.6229 - accuracy: 0.67 - 56s 174ms/step - loss: 0.6226 - accuracy: 0.67 - 56s 174ms/step - loss: 0.6222 - accuracy: 0.67 - 56s 174ms/step - loss: 0.6221 - accuracy: 0.67 - 56s 173ms/step - loss: 0.6216 - accuracy: 0.67 - 57s 173ms/step - loss: 0.6209 - accuracy: 0.67 - 57s 173ms/step - loss: 0.6207 - accuracy: 0.67 - 57s 172ms/step - loss: 0.6206 - accuracy: 0.67 - 57s 172ms/step - loss: 0.6202 - accuracy: 0.68 - 57s 171ms/step - loss: 0.6192 - accuracy: 0.68 - 57s 171ms/step - loss: 0.6188 - accuracy: 0.68 - 57s 171ms/step - loss: 0.6182 - accuracy: 0.68 - 57s 170ms/step - loss: 0.6179 - accuracy: 0.68 - 57s 170ms/step - loss: 0.6175 - accuracy: 0.68 - 57s 170ms/step - loss: 0.6169 - accuracy: 0.6823"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    503/Unknown - 57s 169ms/step - loss: 0.6167 - accuracy: 0.68 - 57s 169ms/step - loss: 0.6160 - accuracy: 0.68 - 57s 169ms/step - loss: 0.6154 - accuracy: 0.68 - 57s 168ms/step - loss: 0.6147 - accuracy: 0.68 - 57s 168ms/step - loss: 0.6143 - accuracy: 0.68 - 57s 168ms/step - loss: 0.6138 - accuracy: 0.68 - 57s 167ms/step - loss: 0.6133 - accuracy: 0.68 - 57s 167ms/step - loss: 0.6130 - accuracy: 0.68 - 58s 167ms/step - loss: 0.6127 - accuracy: 0.68 - 58s 166ms/step - loss: 0.6126 - accuracy: 0.68 - 58s 166ms/step - loss: 0.6121 - accuracy: 0.68 - 58s 166ms/step - loss: 0.6116 - accuracy: 0.68 - 58s 166ms/step - loss: 0.6111 - accuracy: 0.68 - 58s 165ms/step - loss: 0.6107 - accuracy: 0.68 - 58s 165ms/step - loss: 0.6103 - accuracy: 0.68 - 58s 165ms/step - loss: 0.6094 - accuracy: 0.68 - 58s 164ms/step - loss: 0.6089 - accuracy: 0.68 - 58s 164ms/step - loss: 0.6086 - accuracy: 0.68 - 58s 164ms/step - loss: 0.6080 - accuracy: 0.68 - 58s 163ms/step - loss: 0.6075 - accuracy: 0.68 - 58s 163ms/step - loss: 0.6073 - accuracy: 0.68 - 58s 163ms/step - loss: 0.6071 - accuracy: 0.68 - 58s 162ms/step - loss: 0.6068 - accuracy: 0.68 - 58s 162ms/step - loss: 0.6062 - accuracy: 0.68 - 58s 162ms/step - loss: 0.6055 - accuracy: 0.68 - 59s 162ms/step - loss: 0.6048 - accuracy: 0.68 - 59s 161ms/step - loss: 0.6048 - accuracy: 0.69 - 59s 161ms/step - loss: 0.6045 - accuracy: 0.69 - 59s 161ms/step - loss: 0.6043 - accuracy: 0.69 - 59s 160ms/step - loss: 0.6037 - accuracy: 0.69 - 59s 160ms/step - loss: 0.6032 - accuracy: 0.69 - 59s 160ms/step - loss: 0.6027 - accuracy: 0.69 - 59s 160ms/step - loss: 0.6024 - accuracy: 0.69 - 59s 159ms/step - loss: 0.6022 - accuracy: 0.69 - 59s 159ms/step - loss: 0.6016 - accuracy: 0.69 - 59s 159ms/step - loss: 0.6011 - accuracy: 0.69 - 59s 159ms/step - loss: 0.6007 - accuracy: 0.69 - 59s 158ms/step - loss: 0.6004 - accuracy: 0.69 - 59s 158ms/step - loss: 0.5996 - accuracy: 0.69 - 59s 158ms/step - loss: 0.5990 - accuracy: 0.69 - 59s 158ms/step - loss: 0.5987 - accuracy: 0.69 - 59s 157ms/step - loss: 0.5980 - accuracy: 0.69 - 60s 157ms/step - loss: 0.5976 - accuracy: 0.69 - 60s 157ms/step - loss: 0.5969 - accuracy: 0.69 - 60s 157ms/step - loss: 0.5965 - accuracy: 0.69 - 60s 156ms/step - loss: 0.5961 - accuracy: 0.69 - 60s 156ms/step - loss: 0.5956 - accuracy: 0.69 - 60s 156ms/step - loss: 0.5950 - accuracy: 0.69 - 60s 155ms/step - loss: 0.5945 - accuracy: 0.69 - 60s 155ms/step - loss: 0.5944 - accuracy: 0.69 - 60s 155ms/step - loss: 0.5942 - accuracy: 0.69 - 60s 155ms/step - loss: 0.5938 - accuracy: 0.69 - 60s 155ms/step - loss: 0.5939 - accuracy: 0.69 - 60s 154ms/step - loss: 0.5936 - accuracy: 0.69 - 60s 154ms/step - loss: 0.5933 - accuracy: 0.69 - 60s 154ms/step - loss: 0.5927 - accuracy: 0.69 - 60s 154ms/step - loss: 0.5926 - accuracy: 0.69 - 60s 153ms/step - loss: 0.5920 - accuracy: 0.69 - 60s 153ms/step - loss: 0.5921 - accuracy: 0.69 - 60s 153ms/step - loss: 0.5919 - accuracy: 0.69 - 61s 153ms/step - loss: 0.5917 - accuracy: 0.69 - 61s 152ms/step - loss: 0.5910 - accuracy: 0.70 - 61s 152ms/step - loss: 0.5908 - accuracy: 0.70 - 61s 152ms/step - loss: 0.5904 - accuracy: 0.70 - 61s 152ms/step - loss: 0.5904 - accuracy: 0.70 - 61s 151ms/step - loss: 0.5905 - accuracy: 0.70 - 61s 151ms/step - loss: 0.5903 - accuracy: 0.70 - 61s 151ms/step - loss: 0.5900 - accuracy: 0.70 - 61s 151ms/step - loss: 0.5898 - accuracy: 0.70 - 61s 150ms/step - loss: 0.5899 - accuracy: 0.70 - 61s 150ms/step - loss: 0.5893 - accuracy: 0.70 - 61s 150ms/step - loss: 0.5891 - accuracy: 0.70 - 61s 150ms/step - loss: 0.5887 - accuracy: 0.70 - 61s 150ms/step - loss: 0.5884 - accuracy: 0.70 - 61s 149ms/step - loss: 0.5879 - accuracy: 0.70 - 61s 149ms/step - loss: 0.5876 - accuracy: 0.70 - 61s 149ms/step - loss: 0.5869 - accuracy: 0.70 - 62s 149ms/step - loss: 0.5863 - accuracy: 0.70 - 62s 148ms/step - loss: 0.5859 - accuracy: 0.70 - 62s 148ms/step - loss: 0.5857 - accuracy: 0.70 - 62s 148ms/step - loss: 0.5850 - accuracy: 0.70 - 62s 148ms/step - loss: 0.5850 - accuracy: 0.70 - 62s 148ms/step - loss: 0.5849 - accuracy: 0.70 - 62s 147ms/step - loss: 0.5847 - accuracy: 0.70 - 62s 147ms/step - loss: 0.5843 - accuracy: 0.70 - 62s 147ms/step - loss: 0.5839 - accuracy: 0.70 - 62s 147ms/step - loss: 0.5839 - accuracy: 0.70 - 62s 146ms/step - loss: 0.5836 - accuracy: 0.70 - 62s 146ms/step - loss: 0.5832 - accuracy: 0.70 - 62s 146ms/step - loss: 0.5826 - accuracy: 0.70 - 62s 146ms/step - loss: 0.5823 - accuracy: 0.70 - 62s 146ms/step - loss: 0.5827 - accuracy: 0.70 - 62s 145ms/step - loss: 0.5824 - accuracy: 0.70 - 62s 145ms/step - loss: 0.5825 - accuracy: 0.70 - 62s 145ms/step - loss: 0.5820 - accuracy: 0.70 - 63s 145ms/step - loss: 0.5820 - accuracy: 0.70 - 63s 145ms/step - loss: 0.5815 - accuracy: 0.70 - 63s 144ms/step - loss: 0.5811 - accuracy: 0.70 - 63s 144ms/step - loss: 0.5807 - accuracy: 0.70 - 63s 144ms/step - loss: 0.5804 - accuracy: 0.70 - 63s 144ms/step - loss: 0.5802 - accuracy: 0.70 - 63s 144ms/step - loss: 0.5798 - accuracy: 0.70 - 63s 143ms/step - loss: 0.5796 - accuracy: 0.70 - 63s 143ms/step - loss: 0.5795 - accuracy: 0.70 - 63s 143ms/step - loss: 0.5792 - accuracy: 0.70 - 63s 143ms/step - loss: 0.5788 - accuracy: 0.70 - 63s 143ms/step - loss: 0.5784 - accuracy: 0.70 - 63s 142ms/step - loss: 0.5779 - accuracy: 0.70 - 63s 142ms/step - loss: 0.5773 - accuracy: 0.71 - 63s 142ms/step - loss: 0.5769 - accuracy: 0.71 - 63s 142ms/step - loss: 0.5764 - accuracy: 0.71 - 64s 142ms/step - loss: 0.5760 - accuracy: 0.71 - 64s 142ms/step - loss: 0.5757 - accuracy: 0.71 - 64s 141ms/step - loss: 0.5752 - accuracy: 0.71 - 64s 141ms/step - loss: 0.5749 - accuracy: 0.71 - 64s 141ms/step - loss: 0.5746 - accuracy: 0.71 - 64s 141ms/step - loss: 0.5741 - accuracy: 0.71 - 64s 141ms/step - loss: 0.5736 - accuracy: 0.71 - 64s 140ms/step - loss: 0.5733 - accuracy: 0.71 - 64s 140ms/step - loss: 0.5732 - accuracy: 0.71 - 64s 140ms/step - loss: 0.5727 - accuracy: 0.71 - 64s 140ms/step - loss: 0.5720 - accuracy: 0.71 - 64s 140ms/step - loss: 0.5715 - accuracy: 0.71 - 64s 140ms/step - loss: 0.5708 - accuracy: 0.71 - 64s 139ms/step - loss: 0.5704 - accuracy: 0.71 - 64s 139ms/step - loss: 0.5700 - accuracy: 0.71 - 64s 139ms/step - loss: 0.5695 - accuracy: 0.71 - 64s 139ms/step - loss: 0.5693 - accuracy: 0.71 - 65s 139ms/step - loss: 0.5689 - accuracy: 0.71 - 65s 139ms/step - loss: 0.5688 - accuracy: 0.71 - 65s 138ms/step - loss: 0.5683 - accuracy: 0.71 - 65s 138ms/step - loss: 0.5677 - accuracy: 0.71 - 65s 138ms/step - loss: 0.5673 - accuracy: 0.71 - 65s 138ms/step - loss: 0.5670 - accuracy: 0.71 - 65s 138ms/step - loss: 0.5667 - accuracy: 0.71 - 65s 137ms/step - loss: 0.5663 - accuracy: 0.71 - 65s 137ms/step - loss: 0.5659 - accuracy: 0.71 - 65s 137ms/step - loss: 0.5655 - accuracy: 0.71 - 65s 137ms/step - loss: 0.5651 - accuracy: 0.71 - 65s 137ms/step - loss: 0.5645 - accuracy: 0.71 - 65s 137ms/step - loss: 0.5641 - accuracy: 0.71 - 65s 137ms/step - loss: 0.5635 - accuracy: 0.71 - 65s 136ms/step - loss: 0.5634 - accuracy: 0.71 - 65s 136ms/step - loss: 0.5630 - accuracy: 0.71 - 65s 136ms/step - loss: 0.5625 - accuracy: 0.71 - 65s 136ms/step - loss: 0.5621 - accuracy: 0.71 - 66s 136ms/step - loss: 0.5616 - accuracy: 0.71 - 66s 136ms/step - loss: 0.5615 - accuracy: 0.71 - 66s 135ms/step - loss: 0.5612 - accuracy: 0.71 - 66s 135ms/step - loss: 0.5610 - accuracy: 0.71 - 66s 135ms/step - loss: 0.5612 - accuracy: 0.71 - 66s 135ms/step - loss: 0.5610 - accuracy: 0.71 - 66s 135ms/step - loss: 0.5607 - accuracy: 0.71 - 66s 135ms/step - loss: 0.5605 - accuracy: 0.72 - 66s 134ms/step - loss: 0.5601 - accuracy: 0.72 - 66s 134ms/step - loss: 0.5602 - accuracy: 0.72 - 66s 134ms/step - loss: 0.5601 - accuracy: 0.72 - 66s 134ms/step - loss: 0.5603 - accuracy: 0.72 - 66s 134ms/step - loss: 0.5605 - accuracy: 0.72 - 66s 134ms/step - loss: 0.5601 - accuracy: 0.72 - 66s 133ms/step - loss: 0.5600 - accuracy: 0.72 - 66s 133ms/step - loss: 0.5599 - accuracy: 0.72 - 66s 133ms/step - loss: 0.5597 - accuracy: 0.72 - 66s 133ms/step - loss: 0.5595 - accuracy: 0.72 - 67s 133ms/step - loss: 0.5593 - accuracy: 0.72 - 67s 133ms/step - loss: 0.5593 - accuracy: 0.72 - 67s 133ms/step - loss: 0.5591 - accuracy: 0.7208"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    670/Unknown - 67s 132ms/step - loss: 0.5590 - accuracy: 0.72 - 67s 132ms/step - loss: 0.5588 - accuracy: 0.72 - 67s 132ms/step - loss: 0.5584 - accuracy: 0.72 - 67s 132ms/step - loss: 0.5582 - accuracy: 0.72 - 67s 132ms/step - loss: 0.5580 - accuracy: 0.72 - 67s 132ms/step - loss: 0.5577 - accuracy: 0.72 - 67s 131ms/step - loss: 0.5576 - accuracy: 0.72 - 67s 131ms/step - loss: 0.5575 - accuracy: 0.72 - 67s 131ms/step - loss: 0.5574 - accuracy: 0.72 - 67s 131ms/step - loss: 0.5574 - accuracy: 0.72 - 67s 131ms/step - loss: 0.5569 - accuracy: 0.72 - 67s 131ms/step - loss: 0.5566 - accuracy: 0.72 - 67s 131ms/step - loss: 0.5563 - accuracy: 0.72 - 67s 130ms/step - loss: 0.5560 - accuracy: 0.72 - 67s 130ms/step - loss: 0.5556 - accuracy: 0.72 - 68s 130ms/step - loss: 0.5552 - accuracy: 0.72 - 68s 130ms/step - loss: 0.5551 - accuracy: 0.72 - 68s 130ms/step - loss: 0.5547 - accuracy: 0.72 - 68s 130ms/step - loss: 0.5546 - accuracy: 0.72 - 68s 130ms/step - loss: 0.5545 - accuracy: 0.72 - 68s 129ms/step - loss: 0.5542 - accuracy: 0.72 - 68s 129ms/step - loss: 0.5542 - accuracy: 0.72 - 68s 129ms/step - loss: 0.5537 - accuracy: 0.72 - 68s 129ms/step - loss: 0.5534 - accuracy: 0.72 - 68s 129ms/step - loss: 0.5530 - accuracy: 0.72 - 68s 129ms/step - loss: 0.5528 - accuracy: 0.72 - 68s 129ms/step - loss: 0.5525 - accuracy: 0.72 - 68s 128ms/step - loss: 0.5523 - accuracy: 0.72 - 68s 128ms/step - loss: 0.5521 - accuracy: 0.72 - 68s 128ms/step - loss: 0.5518 - accuracy: 0.72 - 68s 128ms/step - loss: 0.5515 - accuracy: 0.72 - 68s 128ms/step - loss: 0.5511 - accuracy: 0.72 - 68s 128ms/step - loss: 0.5508 - accuracy: 0.72 - 69s 128ms/step - loss: 0.5505 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5503 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5500 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5499 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5498 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5494 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5490 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5490 - accuracy: 0.72 - 69s 127ms/step - loss: 0.5487 - accuracy: 0.72 - 69s 126ms/step - loss: 0.5482 - accuracy: 0.72 - 69s 126ms/step - loss: 0.5478 - accuracy: 0.72 - 69s 126ms/step - loss: 0.5475 - accuracy: 0.72 - 69s 126ms/step - loss: 0.5472 - accuracy: 0.72 - 69s 126ms/step - loss: 0.5471 - accuracy: 0.72 - 69s 126ms/step - loss: 0.5468 - accuracy: 0.72 - 69s 126ms/step - loss: 0.5465 - accuracy: 0.72 - 69s 125ms/step - loss: 0.5463 - accuracy: 0.72 - 69s 125ms/step - loss: 0.5459 - accuracy: 0.72 - 69s 125ms/step - loss: 0.5457 - accuracy: 0.72 - 70s 125ms/step - loss: 0.5456 - accuracy: 0.72 - 70s 125ms/step - loss: 0.5453 - accuracy: 0.72 - 70s 125ms/step - loss: 0.5450 - accuracy: 0.72 - 70s 125ms/step - loss: 0.5447 - accuracy: 0.72 - 70s 125ms/step - loss: 0.5444 - accuracy: 0.72 - 70s 125ms/step - loss: 0.5440 - accuracy: 0.72 - 70s 124ms/step - loss: 0.5436 - accuracy: 0.72 - 70s 124ms/step - loss: 0.5432 - accuracy: 0.73 - 70s 124ms/step - loss: 0.5431 - accuracy: 0.73 - 70s 124ms/step - loss: 0.5429 - accuracy: 0.73 - 70s 124ms/step - loss: 0.5428 - accuracy: 0.73 - 70s 124ms/step - loss: 0.5426 - accuracy: 0.73 - 70s 124ms/step - loss: 0.5423 - accuracy: 0.73 - 70s 124ms/step - loss: 0.5419 - accuracy: 0.73 - 70s 123ms/step - loss: 0.5414 - accuracy: 0.73 - 70s 123ms/step - loss: 0.5413 - accuracy: 0.73 - 70s 123ms/step - loss: 0.5412 - accuracy: 0.73 - 71s 123ms/step - loss: 0.5413 - accuracy: 0.73 - 71s 123ms/step - loss: 0.5412 - accuracy: 0.73 - 71s 123ms/step - loss: 0.5409 - accuracy: 0.73 - 71s 123ms/step - loss: 0.5408 - accuracy: 0.73 - 71s 123ms/step - loss: 0.5407 - accuracy: 0.73 - 71s 123ms/step - loss: 0.5406 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5405 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5401 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5397 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5394 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5393 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5391 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5388 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5385 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5382 - accuracy: 0.73 - 71s 122ms/step - loss: 0.5378 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5376 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5373 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5371 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5368 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5366 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5365 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5362 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5359 - accuracy: 0.73 - 72s 121ms/step - loss: 0.5359 - accuracy: 0.73 - 72s 120ms/step - loss: 0.5354 - accuracy: 0.73 - 72s 120ms/step - loss: 0.5352 - accuracy: 0.73 - 72s 120ms/step - loss: 0.5349 - accuracy: 0.73 - 72s 120ms/step - loss: 0.5345 - accuracy: 0.73 - 72s 120ms/step - loss: 0.5345 - accuracy: 0.73 - 72s 120ms/step - loss: 0.5343 - accuracy: 0.73 - 74s 122ms/step - loss: 0.5341 - accuracy: 0.73 - 74s 122ms/step - loss: 0.5338 - accuracy: 0.73 - 74s 122ms/step - loss: 0.5333 - accuracy: 0.73 - 74s 122ms/step - loss: 0.5330 - accuracy: 0.73 - 74s 122ms/step - loss: 0.5328 - accuracy: 0.73 - 74s 122ms/step - loss: 0.5327 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5325 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5322 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5320 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5318 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5319 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5317 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5317 - accuracy: 0.73 - 74s 121ms/step - loss: 0.5315 - accuracy: 0.73 - 75s 121ms/step - loss: 0.5314 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5311 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5310 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5307 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5306 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5305 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5304 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5300 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5301 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5298 - accuracy: 0.73 - 75s 120ms/step - loss: 0.5295 - accuracy: 0.73 - 75s 119ms/step - loss: 0.5291 - accuracy: 0.73 - 75s 119ms/step - loss: 0.5289 - accuracy: 0.73 - 75s 119ms/step - loss: 0.5285 - accuracy: 0.73 - 75s 119ms/step - loss: 0.5282 - accuracy: 0.73 - 75s 119ms/step - loss: 0.5279 - accuracy: 0.73 - 75s 119ms/step - loss: 0.5277 - accuracy: 0.73 - 76s 119ms/step - loss: 0.5274 - accuracy: 0.74 - 76s 119ms/step - loss: 0.5272 - accuracy: 0.74 - 76s 119ms/step - loss: 0.5270 - accuracy: 0.74 - 76s 119ms/step - loss: 0.5269 - accuracy: 0.74 - 76s 119ms/step - loss: 0.5265 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5262 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5262 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5259 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5258 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5256 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5253 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5249 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5247 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5243 - accuracy: 0.74 - 76s 118ms/step - loss: 0.5242 - accuracy: 0.74 - 76s 117ms/step - loss: 0.5239 - accuracy: 0.74 - 76s 117ms/step - loss: 0.5237 - accuracy: 0.74 - 76s 117ms/step - loss: 0.5235 - accuracy: 0.74 - 76s 117ms/step - loss: 0.5233 - accuracy: 0.74 - 78s 119ms/step - loss: 0.5232 - accuracy: 0.74 - 78s 119ms/step - loss: 0.5231 - accuracy: 0.74 - 78s 119ms/step - loss: 0.5229 - accuracy: 0.74 - 78s 119ms/step - loss: 0.5227 - accuracy: 0.74 - 78s 119ms/step - loss: 0.5225 - accuracy: 0.74 - 78s 119ms/step - loss: 0.5222 - accuracy: 0.74 - 78s 118ms/step - loss: 0.5219 - accuracy: 0.74 - 78s 118ms/step - loss: 0.5218 - accuracy: 0.74 - 78s 118ms/step - loss: 0.5213 - accuracy: 0.74 - 78s 118ms/step - loss: 0.5211 - accuracy: 0.74 - 78s 118ms/step - loss: 0.5210 - accuracy: 0.74 - 78s 118ms/step - loss: 0.5206 - accuracy: 0.74 - 79s 118ms/step - loss: 0.5203 - accuracy: 0.74 - 79s 118ms/step - loss: 0.5201 - accuracy: 0.74 - 79s 118ms/step - loss: 0.5198 - accuracy: 0.74 - 79s 118ms/step - loss: 0.5196 - accuracy: 0.74 - 79s 118ms/step - loss: 0.5194 - accuracy: 0.7451"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697/697 [==============================] 0.5192 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5190 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5189 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5189 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5185 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5182 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5180 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5178 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5176 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5172 - accuracy: 0.74 - 79s 117ms/step - loss: 0.5167 - accuracy: 0.74 - 79s 116ms/step - loss: 0.5166 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5164 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5164 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5162 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5161 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5158 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5154 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5152 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5150 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5148 - accuracy: 0.74 - 80s 116ms/step - loss: 0.5144 - accuracy: 0.74 - 80s 115ms/step - loss: 0.5141 - accuracy: 0.74 - 80s 115ms/step - loss: 0.5139 - accuracy: 0.74 - 80s 115ms/step - loss: 0.5135 - accuracy: 0.74 - 80s 115ms/step - loss: 0.5132 - accuracy: 0.74 - 80s 115ms/step - loss: 0.5130 - accuracy: 0.74 - 93s 133ms/step - loss: 0.5130 - accuracy: 0.7492 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "print('\\nEval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
