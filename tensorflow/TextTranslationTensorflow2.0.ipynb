{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "!pip install -q tensorflow==2.0.0-beta1\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 24000, 6000, 6000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "35 ----> soy\n",
      "704 ----> guapa\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> i\n",
      "18 ----> m\n",
      "343 ----> beautiful\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints_translation2.0/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    EPOCHS = 10\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "        # saving (checkpoint) the model every 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20e1ceef128>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-457b7bbda7e8>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jdaaa\\pycharmprojects\\textminingproject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    402\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jdaaa\\pycharmprojects\\textminingproject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jdaaa\\pycharmprojects\\textminingproject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m     \"\"\"\n\u001b[0;32m    588\u001b[0m     return self._call_flat(\n\u001b[1;32m--> 589\u001b[1;33m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[0;32m    590\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m    591\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[1;32mc:\\users\\jdaaa\\pycharmprojects\\textminingproject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jdaaa\\pycharmprojects\\textminingproject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jdaaa\\pycharmprojects\\textminingproject\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
