{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a string s = 'colorless'. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s[0:4] + 'u' + s[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dishes'[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'runing'[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'nationality'[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'undo'[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'preheat'[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-09c95c489ad5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m'data'\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "'data'[-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a \"step\" size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2] Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10][1:10:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 8, 6, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10][10:1:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 7, 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10][10:1:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8 ,9, 10][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 8, 6, 4, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8 ,9, 10][::-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 7, 4, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8 ,9, 10][::-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 6, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8 ,9, 10][::-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "a. [a-zA-Z]+ \n",
    "\n",
    "b. [A-Z][a-z]*\n",
    "\n",
    "c. p[aeiou]{,2}t\n",
    "\n",
    "d. \\d+(\\.\\d+)?\n",
    "\n",
    "e. ([^aeiou][aeiou][^aeiou])*\n",
    "\n",
    "f. \\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'my', 'test', 'staff']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[a-zA-Z]+', 'This is my test staff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[A-Z][a-z]*', 'This is my test staff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['piet']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'p[aeiou]{,2}t', 'This is my test staff with piet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.11', '', '.98', '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\d+(.\\d+)?', 'This is my test staff with number 19.11, 19, 22.98.21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his', 'taf', 'yob', 'san', 'dum']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'([^aeiou][aeiou][^aeiou])+', 'thisismystaffveryobviousanddumb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'test',\n",
       " 'staff',\n",
       " 'with',\n",
       " 'number',\n",
       " '19',\n",
       " '.',\n",
       " '11',\n",
       " ',',\n",
       " '19',\n",
       " ',',\n",
       " '22',\n",
       " '.',\n",
       " '98',\n",
       " '.',\n",
       " '21']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\w+|[^\\w\\s]+', 'This is my test staff with number 19.11, 19, 22.98.21')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write regular expressions to match the following classes of strings:\n",
    "\n",
    "    a. A single determiner (assume that a, an, and the are the only determiners).\n",
    "    b. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'the', 'an']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\s(an|a|the)\\s', 'There was a crazy person, and the another was not, but an anthem was sounding around.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2+3+4*43+99282', '43+')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(([0-9]+[+*]+)*[0-9]+)', '2+3+4*43+99282')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then  request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "response = request.urlopen('http://nltk.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(data, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nNatural Language Toolkit — NLTK 3.4.3 documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNLTK 3.4.3 documentation\\n\\nnext |\\n          modules |\\n          index\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNatural Language Toolkit¶\\nNLTK is a leading platform for building Python programs to work with human language data.\\nIt provides easy-to-use interfaces to over 50 corpora and lexical\\nresources such as WordNet,\\nalong with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\\nwrappers for industrial-strength NLP libraries,\\nand an active discussion forum.\\nThanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\\nNLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\\nNLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\\nNLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\\nand “an amazing library to play with natural language.”\\nNatural Language Processing with Python provides a practical\\nintroduction to programming for language processing.\\nWritten by the creators of NLTK, it guides the reader through the fundamentals\\nof writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\\nand more.\\nThe online version of the book has been been updated for Python 3 and NLTK 3.\\n(The original Python 2 version is still available at http://nltk.org/book_1ed.)\\n\\nSome simple things you can do with NLTK¶\\nTokenize and tag some text:\\n>>> import nltk\\n>>> sentence = \"\"\"At eight o\\'clock on Thursday morning\\n... Arthur didn\\'t feel very good.\"\"\"\\n>>> tokens = nltk.word_tokenize(sentence)\\n>>> tokens\\n[\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\',\\n\\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\']\\n>>> tagged = nltk.pos_tag(tokens)\\n>>> tagged[0:6]\\n[(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'),\\n(\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')]\\n\\n\\nIdentify named entities:\\n>>> entities = nltk.chunk.ne_chunk(tagged)\\n>>> entities\\nTree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'),\\n           (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'),\\n       Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]),\\n           (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'),\\n           (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')])\\n\\n\\nDisplay a parse tree:\\n>>> from nltk.corpus import treebank\\n>>> t = treebank.parsed_sents(\\'wsj_0001.mrg\\')[0]\\n>>> t.draw()\\n\\n\\n\\nNB. If you publish work that uses NLTK, please cite the NLTK book as\\nfollows:\\n\\nBird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\\n\\n\\n\\nNext Steps¶\\n\\nsign up for release announcements\\njoin in the discussion\\n\\n\\n\\n\\nContents¶\\n\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\n\\nIndex\\nModule Index\\nSearch Page\\n\\n\\n\\n\\n\\n\\n\\nTable of Contents\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnext |\\n            modules |\\n            index\\n\\n\\n\\nShow Source\\n\\n\\n\\n\\n        © Copyright 2019, NLTK Project.\\n      Last updated on Jun 06, 2019.\\n      Created using Sphinx 2.0.1.\\n    \\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thi', ' i', ' a very ', 'imple te', 't for ', 'ome other ', 'taff']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'This is a very simple test for some other staff'\n",
    "raw.split('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T,h,i,s, ,i,s, ,a, ,v,e,r,y, ,s,i,m,p,l,e, ,t,e,s,t, ,f,o,r, ,s,o,m,e, ,o,t,h,e,r, ,s,t,a,f,f,"
     ]
    }
   ],
   "source": [
    "for c in raw:\n",
    "    print(c, end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'cde', 'abc', 'jhga', 'jjjdk']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'abc cde\\tabc\\njhga\\tjjjdk'\n",
    "raw.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'cde\\tabc\\njhga\\tjjjdk']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['c', 'd', 'e', 'a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3' * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123456789'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%6s' % ('123456789')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123456789'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%-6s' % ('123456789')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = brown.words()\n",
    "wh_words = [w for w in words if re.search('^wh.*', w.lower())]\n",
    "wh_words = set(wh_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Whaddya',\n",
       " 'Whah',\n",
       " 'Wharf',\n",
       " 'Wharton',\n",
       " 'What',\n",
       " \"What'd\",\n",
       " \"What's\",\n",
       " 'Whatever',\n",
       " 'Whatman',\n",
       " 'Wheat-germ',\n",
       " 'Wheaton',\n",
       " 'Whee',\n",
       " 'Wheel',\n",
       " \"Wheelan's\",\n",
       " 'Wheeler',\n",
       " \"Wheeler's\",\n",
       " 'Wheeling',\n",
       " 'Wheelock',\n",
       " \"Wheelock's\",\n",
       " 'Whelan',\n",
       " 'When',\n",
       " 'Whence',\n",
       " 'Whenever',\n",
       " 'Where',\n",
       " \"Where'd\",\n",
       " \"Where're\",\n",
       " \"Where's\",\n",
       " 'Whereas',\n",
       " 'Wherefore',\n",
       " 'Wherever',\n",
       " 'Whether',\n",
       " 'Which',\n",
       " 'Whichever',\n",
       " 'Whig',\n",
       " 'Whigs',\n",
       " 'While',\n",
       " 'Whimsey',\n",
       " 'Whippet',\n",
       " 'Whipple',\n",
       " \"Whipple's\",\n",
       " 'Whipsnade',\n",
       " 'Whirling',\n",
       " 'Whirlpool',\n",
       " 'Whirlwind',\n",
       " \"Whirlwind's\",\n",
       " 'Whiskey',\n",
       " 'Whisky',\n",
       " 'Whitcomb',\n",
       " 'White',\n",
       " \"White's\",\n",
       " 'White-shirted',\n",
       " 'Whitehall',\n",
       " 'Whitehead',\n",
       " \"Whitehead's\",\n",
       " 'Whiteleaf',\n",
       " 'Whiteley',\n",
       " 'Whiteman',\n",
       " 'Whitemarsh',\n",
       " 'Whitey',\n",
       " 'Whitfield',\n",
       " 'Whiting',\n",
       " 'Whitman',\n",
       " \"Whitman's\",\n",
       " 'Whitney',\n",
       " 'Whitrow',\n",
       " 'Whittaker',\n",
       " 'Whittier',\n",
       " \"Whittier's\",\n",
       " 'Who',\n",
       " \"Who'd\",\n",
       " \"Who'll\",\n",
       " \"Who's\",\n",
       " 'Whoa',\n",
       " 'Whoever',\n",
       " 'Whole',\n",
       " 'Wholesome',\n",
       " 'Whom',\n",
       " 'Whose',\n",
       " 'Whosever',\n",
       " 'Whosoever',\n",
       " 'Why',\n",
       " \"Why'n\",\n",
       " \"Whyn't\",\n",
       " 'whack',\n",
       " 'whacked',\n",
       " 'whaddya',\n",
       " 'whaling',\n",
       " 'wharf',\n",
       " 'wharves',\n",
       " 'what',\n",
       " \"what're\",\n",
       " \"what's\",\n",
       " \"what's-his-name\",\n",
       " 'what-nots',\n",
       " 'what-will-T.',\n",
       " 'whatever',\n",
       " 'whatsoever',\n",
       " 'wheare',\n",
       " 'wheat',\n",
       " 'wheat-germ',\n",
       " 'wheedled',\n",
       " 'wheel',\n",
       " 'wheeled',\n",
       " 'wheeling',\n",
       " 'wheels',\n",
       " 'wheezed',\n",
       " 'wheezes',\n",
       " 'wheezing',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'wher',\n",
       " 'where',\n",
       " 'whereabouts',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'whereever',\n",
       " 'wherefores',\n",
       " 'wherein',\n",
       " 'whereof',\n",
       " 'whereon',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'wherewith',\n",
       " 'whether',\n",
       " 'whetted',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'whichever-the-hell',\n",
       " 'whiff',\n",
       " 'while',\n",
       " 'whim',\n",
       " 'whimper',\n",
       " 'whimpering',\n",
       " 'whims',\n",
       " 'whimsical',\n",
       " 'whine',\n",
       " 'whined',\n",
       " 'whining',\n",
       " 'whinnied',\n",
       " 'whinny',\n",
       " 'whip',\n",
       " \"whip's\",\n",
       " 'whiplash',\n",
       " 'whiplashes',\n",
       " 'whipped',\n",
       " 'whipping',\n",
       " 'whipping-boys',\n",
       " 'whips',\n",
       " 'whipsawed',\n",
       " 'whir',\n",
       " 'whirl',\n",
       " 'whirled',\n",
       " 'whirling',\n",
       " 'whirlwind',\n",
       " 'whirring',\n",
       " 'whisked',\n",
       " 'whiskered',\n",
       " 'whiskers',\n",
       " 'whiskey',\n",
       " 'whisking',\n",
       " 'whisky',\n",
       " 'whisky-on-the-rocks',\n",
       " 'whisper',\n",
       " 'whispered',\n",
       " 'whispering',\n",
       " 'whisperings',\n",
       " 'whispers',\n",
       " 'whistle',\n",
       " 'whistled',\n",
       " 'whistling',\n",
       " 'whit',\n",
       " 'white',\n",
       " 'white-clad',\n",
       " 'white-collar',\n",
       " 'white-columned',\n",
       " 'white-dominated',\n",
       " 'white-stucco',\n",
       " 'white-suited',\n",
       " 'white-topped',\n",
       " 'whiteface',\n",
       " 'whitehaired',\n",
       " 'whitely',\n",
       " 'whitened',\n",
       " 'whiteness',\n",
       " 'whitening',\n",
       " 'whitens',\n",
       " 'whites',\n",
       " 'whitetail',\n",
       " 'whitewashed',\n",
       " 'whiz',\n",
       " 'whizzed',\n",
       " 'whizzing',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " \"who's\",\n",
       " 'whodunnit',\n",
       " 'whoe',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whole-heartedly',\n",
       " 'whole-house',\n",
       " 'whole-wheat',\n",
       " 'whole-word',\n",
       " 'wholeheartedly',\n",
       " 'wholeness',\n",
       " 'wholes',\n",
       " 'wholesale',\n",
       " 'wholesalers',\n",
       " 'wholesome',\n",
       " 'wholewheat',\n",
       " 'wholly',\n",
       " 'wholly-owned',\n",
       " 'whom',\n",
       " 'whoop',\n",
       " 'whooping',\n",
       " 'whoosh',\n",
       " 'whoppers',\n",
       " 'whopping',\n",
       " 'whore',\n",
       " 'whores',\n",
       " 'whorls',\n",
       " 'whose',\n",
       " 'whosoever',\n",
       " 'why',\n",
       " 'whyfores'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "\n",
    "def extrat_text_from_url(url):\n",
    "    text = request.urlopen(url).read()\n",
    "    encoding = chardet.detect(text)\n",
    "    text = text.decode(encoding.get('encoding'))\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extrat_text_from_url(\"https://en.wikipedia.org/wiki/Reference_work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def unkown(url):\n",
    "    text = extrat_text_from_url(url)\n",
    "    words = nltk.corpus.words.words()\n",
    "    words = set(words)\n",
    "    text_words = nltk.regexp_tokenize(text, r'[a-zA-Z]+')\n",
    "    return [w for w in text_words if w not in words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unkown(\"https://en.wikipedia.org/wiki/Reference_work\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13371"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unkown('http://news.bbc.co.uk/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extrat_text_from_url('http://news.bbc.co.uk/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'of', 'tokenization', 'abc', 'abc']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(\"This is a test of tokenization {abc} abc\", r'[a-zA-Z]+|^{.*}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{abc=an}}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show('{.*}', '{abc=an}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var', 'x', 'z']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize('{var x = 10; z = 11}', '[a-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
